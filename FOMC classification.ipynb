{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "481fbc82",
   "metadata": {},
   "source": [
    "# Classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94cc9c58",
   "metadata": {},
   "source": [
    "##  0. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725027dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3ff697bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all the cleanning data\n",
    "fs= pd.read_csv('all_fed_speeches.csv')\n",
    "fs = df.iloc[:, [1, -1]]\n",
    "pm=pd.read_csv('df_minutes.csv')\n",
    "pm.columns = ['date', 'text']\n",
    "pc=pd.read_csv('df_press_conferences.csv')\n",
    "pc.columns = ['date', 'text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "73f7889d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the DataFrames\n",
    "df = pd.concat([fs, pm, pc], ignore_index=True)\n",
    "# Sort by the first column\n",
    "df.sort_values(by=df.columns[0], inplace=True)\n",
    "# Drop rows with any NA values\n",
    "df.dropna(inplace=True)\n",
    "# Optionally, reset the index after dropping rows\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ba7e1bc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011-01-26</td>\n",
       "      <td>The Federal Reserve, the central bank of the U...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011-03-15</td>\n",
       "      <td>The Federal Reserve, the central bank of the U...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011-04-27</td>\n",
       "      <td>The Federal Reserve, the central bank of the U...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2011-06-22</td>\n",
       "      <td>The Federal Reserve, the central bank of the U...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2011-08-09</td>\n",
       "      <td>The Federal Reserve, the central bank of the U...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date                                               text\n",
       "0  2011-01-26  The Federal Reserve, the central bank of the U...\n",
       "1  2011-03-15  The Federal Reserve, the central bank of the U...\n",
       "2  2011-04-27  The Federal Reserve, the central bank of the U...\n",
       "3  2011-06-22  The Federal Reserve, the central bank of the U...\n",
       "4  2011-08-09  The Federal Reserve, the central bank of the U..."
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check the head\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "97ad9813",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(869, 2)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check the shape\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9eb47a",
   "metadata": {},
   "source": [
    "## 1. Word list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e5d92d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define my word list\n",
    "hawkish_words = [\n",
    "    \"inflation\", \"interest rate hike\", \"economic overheating\", \"tightening\", \n",
    "    \"monetary tightening\", \"inflationary pressures\", \"rate increase\", \n",
    "    \"fiscal restraint\", \"price stability\", \"interest rate rise\", \"higher interest rates\", \n",
    "    \"excess demand\", \"inflation expectations\", \"economic overheating\", \"rate normalization\", \n",
    "    \"output gap\", \"asset bubbles\", \"credit tightening\", \"wage pressures\", \n",
    "    \"monetary policy normalization\", \"liquidity constraints\", \"restrictive policy\", \n",
    "    \"policy tightening\", \"balance sheet reduction\", \"bond tapering\", \n",
    "    \"tapering\", \"rising yields\", \"inflation control\", \"monetary contraction\", \n",
    "    \"core inflation\", \"price hikes\", \"higher yields\", \"interest rate ceilings\", \n",
    "    \"currency appreciation\", \"price growth\", \"rate adjustment\", \"debt concerns\", \n",
    "    \"capital flight\", \"policy reversal\", \"currency tightening\", \"price pressures\", \n",
    "    \"overheating risks\", \"recessionary pressures\", \"credit constraints\", \"supply shock\", \n",
    "    \"demand shock\", \"cost-push inflation\", \"supply-side constraints\", \"inflation surge\", \n",
    "    \"labor market overheating\", \"housing bubble\", \"credit risk\", \"overleveraging\", \n",
    "    \"sovereign risk\", \"deleveraging\", \"capital outflows\", \"market overheating\", \n",
    "    \"borrowing costs\", \"fiscal deficit reduction\", \"trade deficit\", \"policy correction\", \n",
    "    \"central bank hawkishness\", \"debt issuance\", \"credit squeeze\", \"inflationary spiral\", \n",
    "    \"bank lending restrictions\", \"financial instability\", \"asset repricing\", \"credit downgrades\", \n",
    "    \"debt tightening\", \"monetary policy shift\", \"higher risk premiums\", \"credit withdrawal\", \n",
    "    \"fiscal adjustment\", \"inflation targeting\", \"rate volatility\", \"real interest rates\", \n",
    "    \"liquidity withdrawal\", \"rate hikes\", \"price controls\", \"yield curves\", \"tight credit\", \n",
    "    \"policy credibility\", \"commodity price surge\", \"import costs\", \"dollar strengthening\", \n",
    "    \"debt servicing\", \"credit rating\", \"reserve requirements\", \"withdrawal of stimulus\", \n",
    "    \"asset bubbles\", \"regulatory tightening\", \"central bank intervention\", \"currency tightening\"\n",
    "    # Add more terms as needed\n",
    "]\n",
    "dovish_words = [\n",
    "    \"lower rates\", \"stimulus\", \"economic growth\", \"accommodative\", \n",
    "    \"quantitative easing\", \"rate cut\", \"fiscal stimulus\", \"monetary expansion\", \n",
    "    \"interest rate cut\", \"expansionary policy\", \"liquidity injection\", \n",
    "    \"stimulus package\", \"growth support\", \"credit easing\", \"policy easing\", \n",
    "    \"rate reduction\", \"fiscal easing\", \"employment growth\", \"credit growth\", \n",
    "    \"labor market recovery\", \"consumer spending\", \"investment incentives\", \n",
    "    \"growth prospects\", \"monetary accommodation\", \"low inflation\", \n",
    "    \"policy accommodation\", \"supportive measures\", \"bond purchases\", \n",
    "    \"balance sheet expansion\", \"inflation tolerance\", \"interest rate reduction\", \n",
    "    \"output expansion\", \"currency devaluation\", \"rate cuts\", \"stimulus measures\", \n",
    "    \"deficit spending\", \"unemployment reduction\", \"credit creation\", \n",
    "    \"negative interest rates\", \"low yield environment\", \"easy money\", \n",
    "    \"liquidity support\", \"quantitative easing measures\", \"demand-side policies\", \n",
    "    \"supply stimulus\", \"credit expansion\", \"soft monetary policy\", \n",
    "    \"supporting growth\", \"debt issuance\", \"market stability\", \"job creation\", \n",
    "    \"consumer price index\", \"housing support\", \"asset purchases\", \n",
    "    \"monetary flexibility\", \"financial support\", \"fiscal expansion\", \"rate accommodation\", \n",
    "    \"currency easing\", \"expansionary monetary policy\", \"market intervention\", \n",
    "    \"fiscal policy boost\", \"credit market support\", \"capital flow management\", \n",
    "    \"fiscal policy adjustment\", \"liquidity measures\", \"low rates\", \"consumer demand\", \n",
    "    \"economic recovery\", \"inflation tolerance\", \"supporting liquidity\", \"employment stimulus\", \n",
    "    \"financial stability\", \"equity market support\", \"unemployment stimulus\", \"budget deficits\", \n",
    "    \"central bank dovishness\", \"credit facilitation\", \"stimulus continuation\", \n",
    "    \"low inflation environment\", \"growth policies\", \"employment growth\", \"debt expansion\", \n",
    "    \"investment stimulus\", \"consumer confidence\", \"trade growth\", \"wage growth\", \n",
    "    \"negative interest rates\", \"policy flexibility\", \"corporate bond purchases\", \n",
    "    \"fiscal stability measures\", \"credit guarantee\", \"currency devaluation\", \n",
    "    \"trade facilitation\", \"debt forgiveness\", \"credit growth measures\", \"foreign direct investment\", \n",
    "    \"trade stimulus\", \"income growth\", \"wealth distribution\", \"market liquidity\", \"capital access\", \n",
    "    \"public sector support\", \"industrial stimulus\", \"expansionary fiscal policy\", \n",
    "    \"accommodative monetary stance\"\n",
    "    # Add more terms as needed\n",
    "]\n",
    "\n",
    "# count the hawkish and dovish word for each text\n",
    "def count_words(text, word_list):\n",
    "    count = sum(text.lower().count(word) for word in word_list)\n",
    "    return count\n",
    "\n",
    "df['dovish_count'] = df['text'].apply(lambda x: count_words(x, dovish_words))\n",
    "df['hawkish_count'] = df['text'].apply(lambda x: count_words(x, hawkish_words))\n",
    "\n",
    "# Clasification according to the count\n",
    "def classify_text(row):\n",
    "    if row['dovish_count'] > row['hawkish_count']*1.5:\n",
    "        return 'dovish'\n",
    "    elif row['hawkish_count'] > row['dovish_count']*1.5:\n",
    "        return 'hawkish'\n",
    "    else:\n",
    "        return 'neutral'\n",
    "\n",
    "df['classification_w'] = df.apply(classify_text, axis=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "41aa1a12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>dovish_count</th>\n",
       "      <th>hawkish_count</th>\n",
       "      <th>classification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011-01-26</td>\n",
       "      <td>The Federal Reserve, the central bank of the U...</td>\n",
       "      <td>7</td>\n",
       "      <td>13</td>\n",
       "      <td>hawkish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011-03-15</td>\n",
       "      <td>The Federal Reserve, the central bank of the U...</td>\n",
       "      <td>6</td>\n",
       "      <td>17</td>\n",
       "      <td>hawkish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011-04-27</td>\n",
       "      <td>The Federal Reserve, the central bank of the U...</td>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "      <td>hawkish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2011-06-22</td>\n",
       "      <td>The Federal Reserve, the central bank of the U...</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>hawkish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2011-08-09</td>\n",
       "      <td>The Federal Reserve, the central bank of the U...</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>hawkish</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date                                               text  \\\n",
       "0  2011-01-26  The Federal Reserve, the central bank of the U...   \n",
       "1  2011-03-15  The Federal Reserve, the central bank of the U...   \n",
       "2  2011-04-27  The Federal Reserve, the central bank of the U...   \n",
       "3  2011-06-22  The Federal Reserve, the central bank of the U...   \n",
       "4  2011-08-09  The Federal Reserve, the central bank of the U...   \n",
       "\n",
       "   dovish_count  hawkish_count classification  \n",
       "0             7             13        hawkish  \n",
       "1             6             17        hawkish  \n",
       "2             6             16        hawkish  \n",
       "3             5             12        hawkish  \n",
       "4             6             12        hawkish  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e6b83fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hawkish: 372\n",
      "Dovish: 350\n",
      "Neutral: 147\n"
     ]
    }
   ],
   "source": [
    "df['classification'] = df.apply(classify_document, axis=1)\n",
    "classification_counts = df['classification'].value_counts()\n",
    "print(classification_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3521828f",
   "metadata": {},
   "source": [
    "## 2. Factor similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55d5b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Load the finbert\n",
    "tokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-tone')\n",
    "model = BertModel.from_pretrained('yiyanghkust/finbert-tone')\n",
    "\n",
    "\n",
    "# Define expanded hawkish and dovish reference phrases\n",
    "hawkish_reference = [\n",
    "    \"inflationary pressures, interest rate hikes, tightening of monetary policy\",\n",
    "    \"aggressive interest rate increases, combating inflation, controlling price stability\",\n",
    "    \"hawkish stance, reducing balance sheet, high inflation concerns\",\n",
    "    \"restrictive monetary policy, tightening liquidity, curbing economic overheating\",\n",
    "    \"higher borrowing costs, maintaining a strong dollar, inflation targeting\",\n",
    "    \"risk of overheating economy, proactive monetary measures, firm interest rate policies\",\n",
    "    \"tightening the money supply, signals of future rate hikes, preemptive measures against inflation\",\n",
    "    \"fighting inflation, reducing demand, restraining economic growth\",\n",
    "    \"stricter lending conditions, focus on price stability, monetary policy normalization\",\n",
    "    \"emphasis on inflation control, balancing growth with inflation risks\"\n",
    "]\n",
    "\n",
    "dovish_reference = [\n",
    "    \"lower rates, economic stimulus, accommodative policy stance\",\n",
    "    \"support for economic growth, easing monetary policy, maintaining low-interest rates\",\n",
    "    \"encouraging investment, promoting consumer spending, softening credit conditions\",\n",
    "    \"stimulus measures, nurturing economic recovery, flexible monetary policy\",\n",
    "    \"prolonged low rates, enhancing liquidity, dovish outlook on inflation\",\n",
    "    \"supporting job growth, minimizing economic disruption, fostering sustainable growth\",\n",
    "    \"favoring accommodative stance, preventing deflationary pressures, emphasis on growth\",\n",
    "    \"lowering borrowing costs, prioritizing economic stability, facilitating easy credit\",\n",
    "    \"expansionary fiscal policy, encouraging financial markets, bolstering consumer confidence\",\n",
    "    \"investing in infrastructure, stimulating demand, creating a supportive economic environment\"\n",
    "]\n",
    "\n",
    "\n",
    "# Embedding and split the long sentence\n",
    "def get_sentence_embedding(sentence):\n",
    "    # Use tokenizer，set truncation=True to split\n",
    "    inputs = tokenizer(sentence, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # use[CLS] token\n",
    "    return outputs.last_hidden_state[:, 0, :].numpy()\n",
    "\n",
    "# Claculating the similarity\n",
    "def calculate_similarity(text, reference_text):\n",
    "    text_embedding = get_sentence_embedding(text)\n",
    "    reference_embedding = get_sentence_embedding(reference_text)\n",
    "    return cosine_similarity(text_embedding, reference_embedding)[0][0]\n",
    "\n",
    "# claculate the hawkish and dovish simnilarity for each text\n",
    "def process_text(text):\n",
    "    try:\n",
    "        hawkish_sim = calculate_similarity(text, hawkish_reference)\n",
    "        dovish_sim = calculate_similarity(text, dovish_reference)\n",
    "        return pd.Series([hawkish_sim, dovish_sim])\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing text: {e}\")\n",
    "        return pd.Series([None, None])\n",
    "\n",
    "# Apply the funtion\n",
    "df[['hawkish_similarity', 'dovish_similarity']] = df['text'].apply(process_text)\n",
    "\n",
    "def classify_document(row):\n",
    "    if pd.isnull(row['hawkish_similarity']) or pd.isnull(row['dovish_similarity']):\n",
    "        return 'unknown'\n",
    "    if row['hawkish_similarity'] > row['dovish_similarity']*1.2:\n",
    "        return 'hawkish'\n",
    "    elif row['dovish_similarity'] > row['hawkish_similarity']*1.2:\n",
    "        return 'dovish'\n",
    "    else:\n",
    "        return 'neutral'\n",
    "\n",
    "df['classification_s'] = df.apply(classify_document, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "ac84f5eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>dovish_count</th>\n",
       "      <th>hawkish_count</th>\n",
       "      <th>classification</th>\n",
       "      <th>classification_numeric</th>\n",
       "      <th>hawkish_similarity</th>\n",
       "      <th>dovish_similarity</th>\n",
       "      <th>classification_s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011-01-26</td>\n",
       "      <td>The Federal Reserve, the central bank of the U...</td>\n",
       "      <td>7</td>\n",
       "      <td>13</td>\n",
       "      <td>hawkish</td>\n",
       "      <td>1</td>\n",
       "      <td>0.449360</td>\n",
       "      <td>0.650187</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011-03-15</td>\n",
       "      <td>The Federal Reserve, the central bank of the U...</td>\n",
       "      <td>6</td>\n",
       "      <td>17</td>\n",
       "      <td>hawkish</td>\n",
       "      <td>1</td>\n",
       "      <td>0.424052</td>\n",
       "      <td>0.622608</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011-04-27</td>\n",
       "      <td>The Federal Reserve, the central bank of the U...</td>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "      <td>hawkish</td>\n",
       "      <td>1</td>\n",
       "      <td>0.424086</td>\n",
       "      <td>0.621737</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2011-06-22</td>\n",
       "      <td>The Federal Reserve, the central bank of the U...</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>hawkish</td>\n",
       "      <td>1</td>\n",
       "      <td>0.503104</td>\n",
       "      <td>0.660841</td>\n",
       "      <td>hawkish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2011-08-09</td>\n",
       "      <td>The Federal Reserve, the central bank of the U...</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>hawkish</td>\n",
       "      <td>1</td>\n",
       "      <td>0.540741</td>\n",
       "      <td>0.665799</td>\n",
       "      <td>hawkish</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date                                               text  dovish_count  \\\n",
       "0 2011-01-26  The Federal Reserve, the central bank of the U...             7   \n",
       "1 2011-03-15  The Federal Reserve, the central bank of the U...             6   \n",
       "2 2011-04-27  The Federal Reserve, the central bank of the U...             6   \n",
       "3 2011-06-22  The Federal Reserve, the central bank of the U...             5   \n",
       "4 2011-08-09  The Federal Reserve, the central bank of the U...             6   \n",
       "\n",
       "   hawkish_count classification  classification_numeric  hawkish_similarity  \\\n",
       "0             13        hawkish                       1            0.449360   \n",
       "1             17        hawkish                       1            0.424052   \n",
       "2             16        hawkish                       1            0.424086   \n",
       "3             12        hawkish                       1            0.503104   \n",
       "4             12        hawkish                       1            0.540741   \n",
       "\n",
       "   dovish_similarity classification_s  \n",
       "0           0.650187          neutral  \n",
       "1           0.622608          neutral  \n",
       "2           0.621737          neutral  \n",
       "3           0.660841          hawkish  \n",
       "4           0.665799          hawkish  "
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "6f16c77f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification_s\n",
      "dovish     379\n",
      "hawkish    353\n",
      "neutral    137\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Count the occurrences of each classification\n",
    "classification_counts = df['classification_s'].value_counts()\n",
    "\n",
    "# Print the counts\n",
    "print(classification_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "aebd0177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hawkish count where both classifications match: 188\n",
      "Dovish count where both classifications match: 190\n",
      "Neutral count where both classifications match: 22\n"
     ]
    }
   ],
   "source": [
    "#Comparison\n",
    "matching_hawkish = ((df['classification'] == 'hawkish') & (df['classification_s'] == 'hawkish')).sum()\n",
    "matching_dovish = ((df['classification'] == 'dovish') & (df['classification_s'] == 'dovish')).sum()\n",
    "matching_neutral = ((df['classification'] == 'neutral') & (df['classification_s'] == 'neutral')).sum()\n",
    "\n",
    "# Print results\n",
    "print(f\"Hawkish count where both classifications match: {matching_hawkish}\")\n",
    "print(f\"Dovish count where both classifications match: {matching_dovish}\")\n",
    "print(f\"Neutral count where both classifications match: {matching_neutral}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "9824de20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the documents\n",
    "df.to_csv('FOMC_classification_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36be52cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
